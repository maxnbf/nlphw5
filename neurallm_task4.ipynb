{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5Yox0gG1DsF"
   },
   "source": [
    "Homework 5: Neural Language Models  (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 4\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names & Sections\n",
    "----\n",
    "Names: __Tijana Cosic (4120) & Max Breslauer-Friedman (4120)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxWlOU9k1DsQ"
   },
   "source": [
    "Task 4: Compare your generated sentences (15 points)\n",
    "----\n",
    "\n",
    "In this task, you'll analyze one of the files that you produced in Task 3. You'll need to compare against the corresponding file that we have provided for you that was generated from the vanilla n-gram language model.\n",
    "\n",
    "Choose *__one__* of the following two options.\n",
    "\n",
    "Option 1: Evaluate the generated words of *character*-based models\n",
    "---\n",
    "\n",
    "Your job for this option is to programmatically measure two things:\n",
    "1. the percentage of words produced by each model that are valid english words.\n",
    "2. the percentage of words produced by each model that are valid english words *and* were not seen at train time.\n",
    "\n",
    "For this task, a word is defined as \"characters between _ \" or \"characters between spaces\" (if you replaced your underscores with spaces when you printed out your new sentences).\n",
    "\n",
    "\n",
    "Make sure to turn in any necessary supporting files along with your submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/maxnbf/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/maxnbf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# your imports here\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "import csv\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import neurallm_utils as nutils\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Valid Words Percentage: 44.53 %\n",
      "Vanilla Valid Words Percentage: 43.12 %\n",
      "Neural Valid & Unseen Words Percentage: 5.2 %\n",
      "Vanilla Valid & Unseen Words Percentage: 5.24 %\n"
     ]
    }
   ],
   "source": [
    "# code here!\n",
    "\n",
    "# abstract into util functions\n",
    "NGRAM = 3 # The ngram language model you want to train\n",
    "EMBEDDING_SAVE_FILE_WORD = \"spooky_embedding_word.txt\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = \"spooky_embedding_char.txt\" # The file to save your word embeddings to\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "\n",
    "# reads in the data tokenized by char and by word\n",
    "data_by_char = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)\n",
    "data_by_word = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=False)\n",
    "\n",
    "# tokenizes by words\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(data_by_word)\n",
    "\n",
    "# loads the english words corpus\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "\n",
    "# converts the words that were seen during training into a set\n",
    "trained_words = set(word_tokenizer.word_index.keys())\n",
    "\n",
    "def is_valid(word):\n",
    "    '''\n",
    "    Checks if a word is a valid English word.\n",
    "    Parameters:\n",
    "    word (str): The word to be checked.\n",
    "    Returns:\n",
    "    bool: True if the word is a valid English word, False otherwise.\n",
    "    '''\n",
    "    \n",
    "    return word.lower() in english_words\n",
    "\n",
    "def is_valid_and_unseen(word):\n",
    "    '''\n",
    "    Checks if a word is both a valid English word and unseen during training.\n",
    "    Parameters:\n",
    "    word (str): The word to be checked.\n",
    "    Returns:\n",
    "    bool: True if the word is both a valid English word and unseen during training, False otherwise.\n",
    "    '''\n",
    "    \n",
    "    return word.lower() in english_words and word.lower() not in trained_words\n",
    "\n",
    "def read_sentences(file_path):\n",
    "    '''\n",
    "    Reads the sentences from the specified file path.\n",
    "    Parameters:\n",
    "    file_path (str): The path of the file.\n",
    "    Returns:\n",
    "    list: A list of sentences in the file.\n",
    "    '''\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read().split('\\n')\n",
    "    \n",
    "def process_sentences(sentences, model_type, trained_words, unseen_words=False):\n",
    "    '''\n",
    "    Processes the sentences and extracts the words.\n",
    "    Parameters:\n",
    "    sentences (list): The sentences to be processed.\n",
    "    model_type (str): The model that the sentences are from.\n",
    "    trained_words (set): The words that were seen during training.\n",
    "    Returns:\n",
    "    lists of tuples: list of valid word tuples, list of invalid word tuples. \n",
    "    '''\n",
    "    \n",
    "    # initializes empty lists for the valid and invalid words\n",
    "    valid_words = []\n",
    "    invalid_words = []\n",
    "    \n",
    "    valid_unseen_count = 0\n",
    "    \n",
    "    # for every sentence, splits it into words and checks if the words are valid or not\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        \n",
    "        for word in words:\n",
    "            if is_valid(word):\n",
    "                valid_words.append((model_type, word))\n",
    "                # if unseen_words == True, counts the number of words that are valid words\n",
    "                # and also unseen at train time\n",
    "                if unseen_words:\n",
    "                    if is_valid_and_unseen(word):\n",
    "                        valid_unseen_count += 1\n",
    "            else:\n",
    "                invalid_words.append((model_type, word))\n",
    "    \n",
    "    return valid_words, invalid_words, valid_unseen_count\n",
    "\n",
    "# loads the sentences from the text files\n",
    "neural_sentences = read_sentences('char_model_sentences.txt')\n",
    "vanilla_sentences = read_sentences('spooky_vanilla_3_char.txt')\n",
    "\n",
    "# processes the sentences from both models\n",
    "neural_valid, neural_invalid, valid_unseen_count_neural = process_sentences(neural_sentences, 'neural', trained_words,\n",
    "                                                                            unseen_words=True)\n",
    "vanilla_valid, vanilla_invalid, valid_unseen_count_vanilla = process_sentences(vanilla_sentences, 'vanilla', trained_words,\n",
    "                                                                               unseen_words=True)\n",
    "\n",
    "# writes the valid words to a CSV file\n",
    "with open('valid_words_lms.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['model', 'sequence'])\n",
    "    writer.writerows(neural_valid + vanilla_valid)\n",
    "\n",
    "# writes the invalid words to a CSV file\n",
    "with open('invalid_words_lms.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['model', 'sequence'])\n",
    "    writer.writerows(neural_invalid + vanilla_invalid)\n",
    "\n",
    "# initializes the total word count for the neural words\n",
    "total_words_neural = 0\n",
    "\n",
    "# loops through each sentence and counts the words\n",
    "for sentence in neural_sentences:\n",
    "    words = sentence.split()\n",
    "    total_words_neural += len(words)\n",
    "\n",
    "# initializes the total word count for the vanilla words\n",
    "total_words_vanilla = 0\n",
    "\n",
    "# loops through each sentence and counts the words\n",
    "for sentence in vanilla_sentences:\n",
    "    words = sentence.split()\n",
    "    total_words_vanilla += len(words)\n",
    "\n",
    "# calculates and prints the percentage of valid words for both models\n",
    "neural_valid_percentage = round((len(neural_valid) / total_words_neural * 100), 2)\n",
    "vanilla_valid_percentage = round((len(vanilla_valid) / total_words_vanilla * 100), 2)\n",
    "\n",
    "print('Neural Valid Words Percentage:', neural_valid_percentage, '%')\n",
    "print('Vanilla Valid Words Percentage:', vanilla_valid_percentage, '%')\n",
    "\n",
    "# calculates and prints the percentage of valid AND unseen words for both models\n",
    "neural_valid_unseen_percentage = round((valid_unseen_count_neural / total_words_neural * 100), 2)\n",
    "vanilla_valid_unseen_percentage = round((valid_unseen_count_vanilla / total_words_vanilla * 100), 2)\n",
    "\n",
    "print('Neural Valid & Unseen Words Percentage:', neural_valid_unseen_percentage, '%')\n",
    "print('Vanilla Valid & Unseen Words Percentage:', vanilla_valid_unseen_percentage, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How did you determine what a valid english word is? __A valid English word is any word that is part of NLTK's word corpus. If it is, then it was considered a valid word; if not, then it was considered an invalid word.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Gather the sequences of characters that are determined not to be words. Sampling at minimum 100 of these sequences, how many of them *should have* been counted as words in your opinion? __5 sequences.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aneandent', 'hich', ',', 'thich', 'triagot', 'brumpled', 'throake', 'smaysidge.', 'lience', 'metionst', 'makented', 'wers.', 'ned', 'mented', 'hoaterd', ',', 'siturs', 'disless', 'usidiumad', 'itue', 'ankgualog', 'nows', 'sity', 'bled', 'ack', 'dournow', 'themakint.', 'dusubjectishat', 'whow', 'wilen', ',', ',', 'hicher', 'ang', 'earsupone,', 'thads', 'tadenespird', 'arber,', 'ond', 'oure', 'whicusearsespectillut', 'faing', 'wasoung;', 'harbe', 'arfaverressend', 'therld', ',', 'woodvively', 'foll', ',', 'gand', 'nottander', 'afenal', 'oung', 'ment', 'antmor', 'disold', 'lor', 'aticke', 'im', 'mom', 'coment', '.', 'stessacted', 'furneasepoes', 'curan,', 'postaked', 'beforedge', 'ang', 'somake', 'ficapproresten', 'theal', 'ehind', 'hemoreauthe', 'erp', 'whis', 'weacelthat', 'hameakingle', 'aguarges', 'hadear', 'terety', 'uposs', 'lairaw;', \"'s\", 'fifte', 'suirent', 'relwas', ',', 'ins', 'dently,', 'wic', 'youlleganclocrucint.', 'usleat', 'tharn', 'dify', ',', 'invisompad', 'thous', 'flosteng', 'marthe']\n"
     ]
    }
   ],
   "source": [
    "# more code here, as needed!\n",
    "\n",
    "# initializes an empty list to store the invalid words\n",
    "invalid_words = []\n",
    "\n",
    "# opens the invalid words CSV file and extracts the invalid words\n",
    "with open('invalid_words_lms.csv', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        invalid_words.append(row[1])\n",
    "\n",
    "# randomly samples 100 invalid words\n",
    "sample_invalid = random.sample(invalid_words, k=100)\n",
    "\n",
    "print(sample_invalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit two csv files alongside this notebook: `valid_words_lms.csv` and `invalid_words_lms.csv`. Both files should have __two__ columns: `model`, `sequence`. `model` will have the value `neural` or `vanilla`. `sequence` will be the corresponding sequence of characters. `valid_words_lms.csv` should contain all sequences from both models you determined to be valid words. `invalid_words_lms.csv` will have all sequences from both models you programatically determined to be invalid words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "wordembeddings_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05041e657fa0436a83611a3d2d345b99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cd0685004814c0d974a1d809e0e2b4f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b0dca775977048f38841afae3d906eb6",
      "value": "100%"
     }
    },
    "140057e9712f46af9ebf5825ef9b1390": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_05041e657fa0436a83611a3d2d345b99",
       "IPY_MODEL_a818afa6bb4f43c8b7e32a3c04f17211",
       "IPY_MODEL_72a47718e310461fbd61b312f7bf7cfe"
      ],
      "layout": "IPY_MODEL_488b55855d4d4ffc8af6d3d77aa3fdf8"
     }
    },
    "150adc7de7f54d63a215482e6a977067": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b93060412f54083b6dd7b9203ae55d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cd0685004814c0d974a1d809e0e2b4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "488b55855d4d4ffc8af6d3d77aa3fdf8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72a47718e310461fbd61b312f7bf7cfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4d9e5b3a1e144e6b34a55ab5cbce43f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_150adc7de7f54d63a215482e6a977067",
      "value": " 19579/19579 [00:00&lt;00:00, 18295.70it/s]"
     }
    },
    "843343b9adc84d949f839d51814d55aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a4d9e5b3a1e144e6b34a55ab5cbce43f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a818afa6bb4f43c8b7e32a3c04f17211": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b93060412f54083b6dd7b9203ae55d0",
      "max": 19579,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_843343b9adc84d949f839d51814d55aa",
      "value": 19579
     }
    },
    "b0dca775977048f38841afae3d906eb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
