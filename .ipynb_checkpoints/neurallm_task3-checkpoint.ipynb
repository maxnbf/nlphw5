{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5: Neural Language Models  (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Feedforward Neural Language Model (60 points)\n",
    "--------------------------\n",
    "\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tijana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing utility functions from Keras\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# necessary\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# optional\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "# if you want fancy progress bars\n",
    "from tqdm import notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# your other imports here\n",
    "import time\n",
    "import numpy as np\n",
    "import neurallm_utils as nutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in necessary data\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# abstract into util functions\n",
    "NGRAM = 3 # The ngram language model you want to train\n",
    "EMBEDDING_SAVE_FILE_WORD = \"spooky_embedding_word.txt\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = \"spooky_embedding_char.txt\" # The file to save your word embeddings to\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "\n",
    "# reads in the data tokenized by char and by word\n",
    "data_by_char = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)\n",
    "data_by_word = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NGRAM = 3 # The ngram language model you want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Tokenizer and fit on your data\n",
    "# do this for both the word and character data\n",
    "\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "char_tokenizer = Tokenizer()\n",
    "char_tokenizer.fit_on_texts(data_by_char)\n",
    "char_encoded = char_tokenizer.texts_to_sequences(data_by_char)\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(data_by_word)\n",
    "word_encoded = word_tokenizer.texts_to_sequences(data_by_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word index for character tokenizer:  60\n",
      "Size of word index for word tokenizer:  25374\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the word index for each of your tokenizers\n",
    "# this should match what you calculated in Task 2 with your embeddings\n",
    "print(\"Size of word index for character tokenizer: \", len(char_tokenizer.word_index))\n",
    "print(\"Size of word index for word tokenizer: \", len(word_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Next, prepare the sequences to train your model from text (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed n-gram based sequences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depening on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process                                    however\n",
    "    process, however                                    afforded\n",
    "    however, afforded\t                                me\n",
    "\n",
    "\n",
    "Our first step is to translate the text into sequences of numbers, \n",
    "one sequence per n-gram window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2957553\n",
      "634080\n"
     ]
    }
   ],
   "source": [
    "def generate_ngram_training_samples(encoded: list, ngram: int) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "\n",
    "    # initializes an empty list to store the generated training samples\n",
    "    samples = []\n",
    "    \n",
    "    # loops through the encoded data and generates the ngrams from the current list\n",
    "    for e in encoded: \n",
    "        temp_list = []\n",
    "        \n",
    "        for i in range(len(e) - ngram + 1):\n",
    "            temp_list.append(e[i:i+ngram])\n",
    "        \n",
    "        # adds the generated ngrams from the current list to the samples list \n",
    "        samples.extend(temp_list)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# generates ngram training samples for both char and word tokenization data\n",
    "char_samples = generate_ngram_training_samples(encoded=char_encoded, ngram=NGRAM)\n",
    "word_samples = generate_ngram_training_samples(encoded=word_encoded, ngram=NGRAM)\n",
    "\n",
    "print(len(char_samples))\n",
    "print(len(word_samples))\n",
    "\n",
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "#\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "# [21, 21, 3]\n",
    "# [21, 3, 9]\n",
    "# [3, 9, 7]\n",
    "# ...\n",
    "# Spooky data by words shoud give 634080 sequences\n",
    "# [1, 1, 32]\n",
    "# [1, 32, 2956]\n",
    "# [32, 2956, 3]\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(634080, 2)\n",
      "(634080,)\n",
      "(2957553, 2)\n",
      "(2957553,)\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here\n",
    "\n",
    "def transform_seq_dimensions(samples: list):\n",
    "    '''\n",
    "    Takes the generated ngram training samples and\n",
    "    splits them into X and y.\n",
    "    Parameters:\n",
    "    samples : list of training samples\n",
    "    return: \n",
    "    X and y\n",
    "    '''\n",
    "    \n",
    "    # initializes both X and y to be empty lists at first\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # for every sample, appends all elements of the current sample except for the last one to X,\n",
    "    # and appends the last element of the current sample to y  \n",
    "    for s in samples:\n",
    "        X.append(s[:-1])\n",
    "        y.append(s[-1])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# transforms both the char and word data into X and y\n",
    "X_char, y_char = transform_seq_dimensions(char_samples)\n",
    "X_word, y_word = transform_seq_dimensions(word_samples)\n",
    "\n",
    "print(np.array(X_word).shape)\n",
    "print(np.array(y_word).shape)\n",
    "print(np.array(X_char).shape)\n",
    "print(np.array(y_char).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Initialize a function that reads the word embeddings you saved earlier\n",
    "# and gives you back mappings from words to their embeddings and also \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def read_embeddings(filename: str, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters:\n",
    "        filename (str): path to file\n",
    "        Tokenizer: tokenizer used to tokenize the data (needed to get the word to index mapping)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    \n",
    "    # loads the embeddings from the specified file\n",
    "    embedding = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "    # initializes empty dictionaries to store the mappings\n",
    "    word_to_vector = {}\n",
    "    index_to_vector = {}\n",
    "    \n",
    "    # iterates through each index and word in the tokenizer's index_word mapping\n",
    "    for index, word in tokenizer.index_word.items():\n",
    "        # gets the vector that corresponds to the current word from the embeddings\n",
    "        vector = embedding[word]\n",
    "        # updates the word_to_vector dictionary with the current word and its vector\n",
    "        word_to_vector[word] = vector\n",
    "        # updates the index_to_vector dictionary with the current index and its vector\n",
    "        index_to_vector[index] = vector\n",
    "\n",
    "    #print(len(word_to_vector.keys()))\n",
    "    #print(len(index_to_vector.keys()))\n",
    "\n",
    "    return word_to_vector, index_to_vector\n",
    "\n",
    "# reads the word embeddings from earlier and gives back their mappings\n",
    "word_to_vector, word_index_to_vector = read_embeddings(EMBEDDING_SAVE_FILE_WORD, word_tokenizer)\n",
    "char_to_vector, char_index_to_vector = read_embeddings(EMBEDDING_SAVE_FILE_CHAR, char_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_', 'e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'd', 'l', 'u', 'm', 'c', 'f', 'w', 'y', 'g', 'p', '<s>', '</s>', ',', 'b', 'v', '.', 'k', ';', '\"', 'x', \"'\", 'q', 'j', 'z', '?', ':', 'Ã©', 'Ã¦', 'Ãª', 'Ã¶', 'Ã¨', 'Ã«', 'Ã ', 'Ã´', 'Ã±', 'Ã¤', 'Ã¯', 'Ã¢', 'Ã¼', 'Î¿', 'á¼¶', 'Î´', 'Î±', 'Ã®', 'Ï…', 'Ï€', 'Î½', 'Ï‚', 'Ã¥', 'Ã§'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NECESSARY FOR CHARACTERS\n",
    "\n",
    "# the \"0\" index of the Tokenizer is assigned for the padding token. Initialize\n",
    "# the vector for padding token as all zeros of embedding size\n",
    "# this adds one to the number of embeddings that were initially saved\n",
    "# (and increases your vocab size by 1)\n",
    "\n",
    "# initializes an empty list for the padding tokens\n",
    "padding_token_vector = []\n",
    "\n",
    "# appends a 0 to the padding_token_vector for each iteration\n",
    "for x in range(EMBEDDINGS_SIZE):\n",
    "    padding_token_vector.append(0)\n",
    "\n",
    "# sets the value of the 0th index in the char_index_to_vector and word_index_to_vector\n",
    "# to the padding_token_vector\n",
    "char_index_to_vector[0] = padding_token_vector\n",
    "word_index_to_vector[0] = padding_token_vector\n",
    "\n",
    "# gets the keys of the char_to_vector dictionary\n",
    "char_to_vector.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, index_2_embedding: dict) -> (list, list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    If for_feedforward is True: \n",
    "    Returns data generator to be used by feed_forward\n",
    "    else: Returns data generator for RNN model\n",
    "    '''\n",
    "    \n",
    "    # initializes the index for iterating through the data\n",
    "    index = 0\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        # this is the data in the form [[21, 21], [21, 3], [3, 9], ...]\n",
    "        embeddings = []\n",
    "        \n",
    "        # sets the starting and ending points for the current batch\n",
    "        start = index\n",
    "        end = min(index+num_sequences_per_batch, len(X))\n",
    "\n",
    "        # loops through the indices in X for the current batch\n",
    "        for indices in X[start:end]:\n",
    "            # initializes an empty list to store the current row of embeddings\n",
    "            row = []\n",
    "            \n",
    "            # extends the row with the corresponding embedding for each index  \n",
    "            for i in indices:  \n",
    "                row.extend(index_2_embedding[i])\n",
    "\n",
    "            # appends the row to the embeddings list for the current batch\n",
    "            embeddings.append(row)\n",
    "\n",
    "        vocab_len = len(index_2_embedding.keys())\n",
    "        \n",
    "        # uses one-hot encoding to represent the class labels\n",
    "        labels = to_categorical(y[start:end], num_classes=vocab_len)\n",
    "        \n",
    "        # reshape maintains the correct shape\n",
    "        yield (np.array(embeddings).reshape(len(embeddings), EMBEDDINGS_SIZE*(NGRAM-1)), np.array(labels))\n",
    "        \n",
    "        # updates the index for the next batch and ensures it doesn't go over the length of X\n",
    "        index += num_sequences_per_batch\n",
    "        index %= len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4953\n",
      "(128, 100)\n",
      "(128, 25375)\n",
      "(128, 100)\n",
      "(128, 61)\n",
      "23105 4953\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# initialize your data_generator for both word and character data\n",
    "# print out the shapes of the first batch to verify that it is correct for both word and character data\n",
    "\n",
    "# Examples:\n",
    "# num_sequences_per_batch = 128 # this is the batch size\n",
    "# steps_per_epoch = len(sequences)//num_sequences_per_batch # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE) (128, 200)\n",
    "# sample[1].shape # (batch_size, |V|) to_categorical\n",
    "\n",
    "# word data\n",
    "num_sequences_per_batch = 128 # this is the batch size\n",
    "steps_per_epoch_word = len(X_word)//num_sequences_per_batch # Number of batches per epoch\n",
    "train_generator_word = data_generator(X_word, y_word, num_sequences_per_batch, word_index_to_vector)\n",
    "\n",
    "print(steps_per_epoch_word)\n",
    "\n",
    "sample = next(train_generator_word) # this is how you get data out of generators\n",
    "\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)\n",
    "\n",
    "#sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE) (128, 200)\n",
    "#sample[1].shape # (batch_size, |V|) to_categorical\n",
    "\n",
    "# character data\n",
    "steps_per_epoch_char = len(X_char)//num_sequences_per_batch # Number of batches per epoch\n",
    "train_generator_char = data_generator(X_char, y_char, num_sequences_per_batch, char_index_to_vector)\n",
    "\n",
    "sample = next(train_generator_char) # this is how you get data out of generators\n",
    "\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)\n",
    "\n",
    "print(steps_per_epoch_char, steps_per_epoch_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Train & __save__ your models (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 25375)             1294125   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,301,725\n",
      "Trainable params: 1,301,725\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 61)                3111      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,711\n",
      "Trainable params: 10,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 15 points \n",
    "\n",
    "# code to train a feedforward neural language model for \n",
    "# both word embeddings and character embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "# (define functions as needed)\n",
    "\n",
    "# train your models for between 3 & 5 epochs\n",
    "# on Felix's machine, this takes ~ 24 min for character embeddings and ~ 10 min for word embeddings\n",
    "# DO NOT EXPECT ACCURACIES OVER 0.5 (and even that is very for this many epochs)\n",
    "# We recommend starting by training for 1 epoch\n",
    "\n",
    "# Define your model architecture using Keras Sequential API\n",
    "# Use the adam optimizer instead of sgd\n",
    "# add cells as desired\n",
    "\n",
    "def build_nn(input_size: int, vocab_size: int) -> Sequential:\n",
    "    '''\n",
    "    Builds a neural network model.\n",
    "\n",
    "    Parameters:\n",
    "    input_size (int): The size of the input layer.\n",
    "    vocab_size (int): The size of the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "    Sequential: A Keras Sequential model.\n",
    "    '''\n",
    "    \n",
    "    # initializes a Sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # adds hidden layers\n",
    "    # or input_shape=(input_size,)\n",
    "    model.add(Dense(units=50, activation=\"elu\", input_dim=input_size))\n",
    "    model.add(Dense(units=50, activation=\"elu\"))\n",
    "\n",
    "    # adds an output layer\n",
    "    # do i need units = vocab_size, does output and input need to match for multiple epochs?\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "    # shows the model's verbose\n",
    "    model.summary()\n",
    "\n",
    "    # calls compile here\n",
    "    # https://stackoverflow.com/questions/45632549/why-is-the-accuracy-for-my-keras-model-always-0-when-training\n",
    "    # different metric?\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# builds the word and character neural models\n",
    "word_model = build_nn((NGRAM-1) * EMBEDDINGS_SIZE, len(word_index_to_vector.keys()))\n",
    "char_model = build_nn((NGRAM-1) * EMBEDDINGS_SIZE, len(char_index_to_vector.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25375"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index_to_vector.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4953/4953 [==============================] - 473s 95ms/step - loss: 5.7419 - accuracy: 0.1883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20b94d697c0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is some example code to train a model with a data generator\n",
    "# model.fit(x=train_generator, \n",
    "#           steps_per_epoch=steps_per_epoch,\n",
    "#           epochs=1)\n",
    "\n",
    "word_model.fit(x=train_generator_word, \n",
    "               steps_per_epoch=steps_per_epoch_word,\n",
    "               epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23105/23105 [==============================] - 194s 8ms/step - loss: 2.0547 - accuracy: 0.3773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20b8dac6a30>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_model.fit(x=train_generator_char, \n",
    "               steps_per_epoch=steps_per_epoch_char,\n",
    "               epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spooky data model by character for 5 epochs takes ~ 24 min on Felix's computer\n",
    "# with adam optimizer, gets accuracy of 0.3920\n",
    "\n",
    "# spooky data model by word for 5 epochs takes 10 min on Felix's computer\n",
    "# results in accuracy of 0.2110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your trained models so you can re-load instead of re-training each time\n",
    "# also, you'll need these to generate your sentences!\n",
    "char_model.save(\"char_model.h5\")\n",
    "word_model.save(\"word_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Generate Sentences (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your models if you need to\n",
    "word_model = keras.models.load_model(\"word_model.h5\")\n",
    "char_model = keras.models.load_model(\"char_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# generate a sequence from the model until you get an end of sentence token\n",
    "# This is an example function header you might use\n",
    "def generate_seq(model: Sequential, tokenizer: Tokenizer, seed: list):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    \n",
    "    sentence = seed.copy()\n",
    "\n",
    "    while True:\n",
    "        encoded = tokenizer.texts_to_sequences([\" \".join(sentence)])[0]\n",
    "\n",
    "        # to match input dimension of the NN\n",
    "        padded = pad_sequences([encoded], maxlen=(NGRAM-1)*EMBEDDINGS_SIZE, padding='pre')\n",
    "        prediction = model.predict(padded, verbose=0)\n",
    "        print(prediction)\n",
    "        # gets the index randomly based on the probabilities from the model's prediction\n",
    "        index = np.random.choice(len(prediction[0]), 1, p=prediction[0])[0]\n",
    "        #index = np.argmax(prediction)\n",
    "\n",
    "        next_word = tokenizer.index_word[index]\n",
    "        sentence.append(next_word)\n",
    "\n",
    "        print(\" \".join(sentence))\n",
    "        \n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "    \n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "<s> <s> I like eating food that tastes left\n",
      "[[0. 0. 1. ... 0. 0. 0.]]\n",
      "<s> <s> I like eating food that tastes left </s>\n",
      "<s> <s> I like eating food that tastes left </s>\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "<s> <s> Go away crabbed\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "<s> <s> Go away crabbed first\n",
      "[[0. 0. 1. ... 0. 0. 0.]]\n",
      "<s> <s> Go away crabbed first </s>\n",
      "<s> <s> Go away crabbed first </s>\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# generate and display one sequence from both the word model and the character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "\n",
    "# you may leave _ as _ or replace it with a space if you prefer\n",
    "#print(generate_seq(word_model, word_tokenizer, [\"<s>\", \"<s>\"]))\n",
    "print(generate_seq(word_model, word_tokenizer, [\"<s>\", \"<s>\", \"I\", \"like\", \"eating\", \"food\", \"that\", \"tastes\"]))\n",
    "print(generate_seq(word_model, word_tokenizer, [\"<s>\", \"<s>\", \"Go\", \"away\"]))\n",
    "#print(generate_seq(word_model, word_tokenizer, [\"<s>\", \"<s>\", \"Go\", \"now\", \"to\", \"the\"]))\n",
    "#print(generate_seq(char_model, char_tokenizer, [\"<s>\", \"<s>\", \"I\", \"_\", \"l\", \"i\", \"k\", \"e\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 100 example sentences with each model and save them to a file, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
