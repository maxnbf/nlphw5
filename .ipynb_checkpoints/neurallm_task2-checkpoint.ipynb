{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5: Neural Language Models  (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Training your own word embeddings (14 points)\n",
    "--------------------------------\n",
    "\n",
    "For this task, you'll use the `gensim` package to train your own embeddings for both words and characters. These will eventually act as inputs to your neural language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tijana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import your libraries here\n",
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils as nutils\n",
    "\n",
    "# for word embeddings\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "\n",
    "NGRAM = 3 # The ngram language model you want to train\n",
    "EMBEDDING_SAVE_FILE_WORD = \"spooky_embedding_word.txt\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = \"spooky_embedding_char.txt\" # The file to save your word embeddings to\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "\n",
    "# The dimensions of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# DO NOT WRITE \"50\" WHEN YOU ARE REFERRING TO THE EMBEDDING SIZE\n",
    "EMBEDDINGS_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train embeddings on provided dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two sentences in CSV split by character: ['<s>', '<s>', 'i', 'n', '_', 'h', 'i', 's', '_', 'l', 'e', 'f', 't', '_', 'h', 'a', 'n', 'd', '_', 'w', 'a', 's', '_', 'a', '_', 'g', 'o', 'l', 'd', '_', 's', 'n', 'u', 'f', 'f', '_', 'b', 'o', 'x', ',', '_', 'f', 'r', 'o', 'm', '_', 'w', 'h', 'i', 'c', 'h', ',', '_', 'a', 's', '_', 'h', 'e', '_', 'c', 'a', 'p', 'e', 'r', 'e', 'd', '_', 'd', 'o', 'w', 'n', '_', 't', 'h', 'e', '_', 'h', 'i', 'l', 'l', ',', '_', 'c', 'u', 't', 't', 'i', 'n', 'g', '_', 'a', 'l', 'l', '_', 'm', 'a', 'n', 'n', 'e', 'r', '_', 'o', 'f', '_', 'f', 'a', 'n', 't', 'a', 's', 't', 'i', 'c', '_', 's', 't', 'e', 'p', 's', ',', '_', 'h', 'e', '_', 't', 'o', 'o', 'k', '_', 's', 'n', 'u', 'f', 'f', '_', 'i', 'n', 'c', 'e', 's', 's', 'a', 'n', 't', 'l', 'y', '_', 'w', 'i', 't', 'h', '_', 'a', 'n', '_', 'a', 'i', 'r', '_', 'o', 'f', '_', 't', 'h', 'e', '_', 'g', 'r', 'e', 'a', 't', 'e', 's', 't', '_', 'p', 'o', 's', 's', 'i', 'b', 'l', 'e', '_', 's', 'e', 'l', 'f', '_', 's', 'a', 't', 'i', 's', 'f', 'a', 'c', 't', 'i', 'o', 'n', '.', '</s>', '</s>']\n",
      "First two sentences in CSV split by word: ['<s>', '<s>', 'in', 'his', 'left', 'hand', 'was', 'a', 'gold', 'snuff', 'box', ',', 'from', 'which', ',', 'as', 'he', 'capered', 'down', 'the', 'hill', ',', 'cutting', 'all', 'manner', 'of', 'fantastic', 'steps', ',', 'he', 'took', 'snuff', 'incessantly', 'with', 'an', 'air', 'of', 'the', 'greatest', 'possible', 'self', 'satisfaction', '.', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# use the provided utility functions to read in the data\n",
    "data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "# reads in the data both by character and by word\n",
    "data_by_char = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)\n",
    "data_by_word = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=False)\n",
    "\n",
    "# prints out the first two sentences in each format\n",
    "# make sure we can read the output easily, but you are welcome to leave tokens in their lists\n",
    "print(\"First two sentences in CSV split by character:\", data_by_char[2])\n",
    "print(\"First two sentences in CSV split by word:\", data_by_word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=60, vector_size=50, alpha=0.025>\n",
      "Word2Vec<vocab=25374, vector_size=50, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# 10 points\n",
    "# create your word embeddings\n",
    "# use the skip gram algorithm and a window size of 5\n",
    "# min_count should be 1\n",
    "# takes ~3 sec on Felix's computer for character embeddings using skip-gram with window size 5\n",
    "# takes ~3 sec on Felix's computer for word embeddings using skip-gram with window size 5\n",
    "char_model = Word2Vec(sentences=data_by_char, window=5, min_count=1, vector_size=EMBEDDINGS_SIZE)\n",
    "word_model = Word2Vec(sentences=data_by_word, window=5, min_count=1, vector_size=EMBEDDINGS_SIZE)\n",
    "print(char_model)\n",
    "print(word_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for char embeddings:  60\n",
      "Vocab size for word embeddings:  25374\n"
     ]
    }
   ],
   "source": [
    "# 4 points\n",
    "# print out the vocabulary size for your embeddings for both your word\n",
    "# embeddings and your character embeddings\n",
    "# label which is which when you print them out\n",
    "\n",
    "print(\"Vocab size for char embeddings: \", len(char_model.wv))\n",
    "print(\"Vocab size for word embeddings: \", len(word_model.wv))\n",
    "\n",
    "# Vocabulary size for character embeddings is 60\n",
    "# Vocabulary size for word embeddings is 25374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both sets of embeddings in txt format\n",
    "# use the save_word2vec_format method to accomplish this, with the flag binary=False\n",
    "# you will load your embeddings from files later in the assignment\n",
    "# and can do so whenever you want to not re-train your embeddings\n",
    "# look at these files to make sure that you understand their format\n",
    "\n",
    "char_model.wv.save_word2vec_format(EMBEDDING_SAVE_FILE_CHAR, binary=False)\n",
    "word_model.wv.save_word2vec_format(EMBEDDING_SAVE_FILE_WORD, binary=False)\n",
    "\n",
    "# Saving file in txt format. This will be used later in Sections 4 and 5\n",
    "# make it so that you don't have to re-train the embeddings each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "CS 6120 REQUIRED: Visualize your word embeddings\n",
    "----\n",
    "\n",
    "Visualizing word embeddings is notoriously difficult because they are high-dimensional. Two typical ways of visualizing them involve projecting them into two dimensions, then graphing their projected locations. Two algorithms that do this sort of projection are t-SNE and PCA. You can also use a clustering algorithm to accomplish this visualization. Examples of this are in your text in section 6.9.\n",
    "\n",
    "\n",
    "Using one of these algorithms--either projection or clustering--(you may use a library's implementation), make visualizations of both your word embeddings and your character embeddings.\n",
    "\n",
    "It is up to you to make sure that your visualizations are __both__ legible and meaningful. Legible indicates that when someone looks at them, they are easy to interpret. Meaningful indicates that these visualizations communication something (interesting) about your embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the meaning that you hope to communicate with your visualization of your word embeddings?\n",
    "\n",
    "2. What is the meaning that you hope to communicate with your visualization of your character embeddings?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
